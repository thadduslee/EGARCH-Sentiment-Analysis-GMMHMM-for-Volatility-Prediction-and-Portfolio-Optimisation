{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b369653b-c5b0-4b1c-9a06-b2ae2fcfca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 291/291 [00:11<00:00, 25.85it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "peft_model = \"../outputs/lora_int8_sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float16,     # <-- key line\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True, \n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe00308-b21f-4b1d-baf1-0453d947c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sentiment_probs(input_sentence, labels, max_length=2048):\n",
    "    prompt_template = [\n",
    "        f\"\"\"Instruction: What is the sentiment of this news? Please choose an answer from {{negative/neutral/positive}}\n",
    "        Input: {input_sentence}\n",
    "        Answer:\"\"\"\n",
    "    ]\n",
    "    # tokenize prompt\n",
    "    prompt_tok = tokenizer(\n",
    "        prompt_template, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "    )\n",
    "    input_ids_prompt = prompt_tok[\"input_ids\"].to(device)\n",
    "    attn_prompt = prompt_tok[\"attention_mask\"].to(device)\n",
    "\n",
    "    B = input_ids_prompt.size(0)\n",
    "    C = len(labels)\n",
    "\n",
    "    # pre-tokenize the labels for comparison\n",
    "    label_ids_list = [\n",
    "        tokenizer(l, add_special_tokens=False).input_ids for l in labels\n",
    "    ]\n",
    "\n",
    "    # Compute scores: shape [B, C]\n",
    "    scores = torch.empty((B, C), device=device, dtype=torch.float32)\n",
    "\n",
    "    for ci, lab_ids in enumerate(label_ids_list):\n",
    "        lab = torch.tensor(lab_ids, device=device).unsqueeze(0).repeat(B, 1)  # [B, L]\n",
    "        L = lab.size(1)\n",
    "\n",
    "        # Build combined input: [prompt, label]\n",
    "        input_ids = torch.cat([input_ids_prompt, lab], dim=1)  # [B, P+L]\n",
    "        attn = torch.cat([attn_prompt, torch.ones((B, L), device=device, dtype=attn_prompt.dtype)], dim=1)\n",
    "\n",
    "        # Forward\n",
    "        out = model(input_ids=input_ids, attention_mask=attn)\n",
    "        logits = out.logits  # [B, P+L, V]\n",
    "\n",
    "        # We want log p(label_token_i | prompt + previous label tokens)\n",
    "        # The probability for token at position t is predicted from logits at position t-1.\n",
    "        P = input_ids_prompt.size(1)\n",
    "        # label tokens are at positions [P, P+L-1]\n",
    "        # their predictors are logits at positions [P-1, P+L-2]\n",
    "        pred_positions = torch.arange(P - 1, P + L - 1, device=device)  # length L\n",
    "        label_positions = torch.arange(P, P + L, device=device)         # length L\n",
    "\n",
    "        # Gather logprobs for the label tokens\n",
    "        logprobs = F.log_softmax(logits[:, pred_positions, :], dim=-1)  # [B, L, V]\n",
    "        target = input_ids[:, label_positions]                          # [B, L]\n",
    "        token_logp = logprobs.gather(-1, target.unsqueeze(-1)).squeeze(-1)  # [B, L]\n",
    "\n",
    "        # Mask out any padding in the prompt doesn't matter here; label tokens are always \"real\"\n",
    "        # Length-normalize to reduce short-label bias\n",
    "        scores[:, ci] = token_logp.mean(dim=1)\n",
    "\n",
    "    probs = F.softmax(scores, dim=1)  # [B, C]\n",
    "    return scores, probs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3489aa-5da8-4c8d-ab7f-d4a54a0dc2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: see if the current model can perform well at our task: \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('../labelled_posts.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2fa4f1-d161-4454-adbc-2b9023fdd51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================Combined text:========================================\n",
      "MSTR Puts\n",
      "\n",
      "Opened a couple of weeks ago.. was hoping for bitcoin to crash in Q3.. things happened much faster. I think the floor will be around 35-40K for Bitcoin.\n",
      "\n",
      "not this one.. don't see where the 25% will come from with the price of bitcoin\n",
      "========================================GT Sentiment:========================================\n",
      "negative\n",
      "========================================Model Answer::========================================\n",
      "  negative: 0.8609\n",
      "  neutral : 0.1373\n",
      "  positive: 0.0018\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print('='*40 + 'Combined text:' + '='*40)\n",
    "combined_text = df.iloc[idx]['combined_text']\n",
    "print(df.iloc[idx]['combined_text'])\n",
    "\n",
    "\n",
    "print('='*40 + 'GT Sentiment:' + '='*40)\n",
    "print(df.iloc[idx]['sentiment'])\n",
    "\n",
    "print('='*40 + 'Model Answer::' + '='*40)\n",
    "scores, probs = sentiment_probs(combined_text, labels)\n",
    "\n",
    "\n",
    "for i, p in enumerate(probs.tolist()):\n",
    "    for lab, pr in zip(labels, p):\n",
    "        print(f\"  {lab:8s}: {pr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71af4fd3-ffbf-4b1a-b7d2-56a5044d4690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1275/1275 [08:22<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Total samples: 1275\n",
      "Correct: 1073\n",
      "Accuracy: 0.8416\n",
      "Num wrong: 202\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct = 0\n",
    "total = len(df)\n",
    "\n",
    "all_preds = []\n",
    "all_gt = []\n",
    "wrong_cases = []   # <-- store mistakes here\n",
    "\n",
    "for idx in tqdm(range(total)):\n",
    "    combined_text = df.iloc[idx]['combined_text']\n",
    "    gt = df.iloc[idx]['sentiment'].strip().lower()\n",
    "\n",
    "    scores, probs = sentiment_probs(combined_text, labels)\n",
    "\n",
    "    pred_idx = torch.argmax(probs, dim=1).item()\n",
    "    pred_label = labels[pred_idx]\n",
    "\n",
    "    all_preds.append(pred_label)\n",
    "    all_gt.append(gt)\n",
    "\n",
    "    if pred_label == gt:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong_cases.append({\n",
    "            \"idx\": idx,\n",
    "            \"text\": combined_text,\n",
    "            \"ground_truth\": gt,\n",
    "            \"prediction\": pred_label,\n",
    "            \"probs\": {lab: float(probs[0][i]) for i, lab in enumerate(labels)}\n",
    "        })\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {total}\")\n",
    "print(f\"Correct: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Num wrong: {len(wrong_cases)}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8d94-2f5f-497d-811e-9e79ef11f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or.... split dataset manually \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "nfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
